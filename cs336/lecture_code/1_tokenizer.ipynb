{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "from abc import ABC\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "class Tokenizer(ABC):\n",
    "    \"\"\"Abstract interface for a tokenizer.\"\"\"\n",
    "    def encode(self, string: str) -> list[int]:\n",
    "        raise NotImplementedError\n",
    "    def decode(self, indices: list[int]) -> str:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "def get_compression_ratio(string: str, indices: list[int]) -> float:\n",
    "    \"\"\"Given `string` that has been tokenized into `indices`, .\"\"\"\n",
    "    num_bytes = len(bytes(string, encoding=\"utf-8\")) \n",
    "    num_tokens = len(indices)                       \n",
    "    return num_bytes / num_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert ord(\"a\") == 97\n",
    "assert ord(\"游깴\") == 127757\n",
    "\n",
    "assert chr(97) == \"a\"\n",
    "assert chr(127757) == \"游깴\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string: Hello, 游깴! 擔먼봏!\n",
      "indices: [72, 101, 108, 108, 111, 44, 32, 127757, 33, 32, 20320, 22909, 33]\n",
      "compression_ratio: 1.5384615384615385\n"
     ]
    }
   ],
   "source": [
    "class CharacterTokenizer(Tokenizer):\n",
    "    \"\"\"Represent a string as a sequence of Unicode code points.\"\"\"\n",
    "    def encode(self, string: str) -> list[int]:\n",
    "        return list(map(ord, string))\n",
    "    def decode(self, indices: list[int]) -> str:\n",
    "        return \"\".join(map(chr, indices))\n",
    "\n",
    "tokenizer = CharacterTokenizer()\n",
    "string = \"Hello, 游깴! 擔먼봏!\"  \n",
    "print(f\"string: {string}\")  # @inspect string\n",
    "indices = tokenizer.encode(string) \n",
    "print(f\"indices: {indices}\")  # @inspect indices\n",
    "reconstructed_string = tokenizer.decode(indices) \n",
    "assert string == reconstructed_string\n",
    "\n",
    "vocabulary_size = max(indices) + 1  # This is a lower bound\n",
    "compression_ratio = get_compression_ratio(string, indices)\n",
    "print(f\"compression_ratio: {compression_ratio}\")  # @inspect compression_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Byte Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert bytes(\"a\", encoding=\"utf-8\") == b\"a\"\n",
    "assert bytes(\"游깴\", encoding=\"utf-8\") == b\"\\xf0\\x9f\\x8c\\x8d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[72]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(int, b'H'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string: Hello, 游깴! 擔먼봏!\n",
      "string_bytes: b'Hello, \\xf0\\x9f\\x8c\\x8d! \\xe4\\xbd\\xa0\\xe5\\xa5\\xbd!'\n",
      "indices: [72, 101, 108, 108, 111, 44, 32, 240, 159, 140, 141, 33, 32, 228, 189, 160, 229, 165, 189, 33]\n"
     ]
    }
   ],
   "source": [
    "class ByteTokenizer(Tokenizer):\n",
    "    \"\"\"Represent a string as a sequence of bytes.\"\"\"\n",
    "    def encode(self, string: str) -> list[int]:\n",
    "        string_bytes = string.encode(\"utf-8\")  \n",
    "        print(f\"string_bytes: {string_bytes}\")  # @inspect string_bytes\n",
    "        indices = list(map(int, string_bytes))  \n",
    "        return indices\n",
    "    \n",
    "    def decode(self, indices: list[int]) -> str:\n",
    "        string_bytes = bytes(indices)  \n",
    "        string = string_bytes.decode(\"utf-8\")\n",
    "        return string\n",
    "    \n",
    "tokenizer = ByteTokenizer()\n",
    "string = \"Hello, 游깴! 擔먼봏!\"  \n",
    "print(f\"string: {string}\")  # @inspect string\n",
    "indices = tokenizer.encode(string)  \n",
    "print(f\"indices: {indices}\")  # @inspect indices\n",
    "reconstructed_string = tokenizer.decode(indices)\n",
    "assert string == reconstructed_string\n",
    "\n",
    "vocabulary_size = 256\n",
    "compression_ratio = get_compression_ratio(string, indices)\n",
    "assert compression_ratio == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"'\", 'll', ' ', 'say', ' ', 'supercalifragilisticexpialidocious', '!']\n",
      "['I', \"'ll\", ' say', ' supercalifragilisticexpialidocious', '!']\n"
     ]
    }
   ],
   "source": [
    "string = \"I'll say supercalifragilisticexpialidocious!\"\n",
    "\n",
    "segments = regex.findall(r\"\\w+|.\", string) \n",
    "print(segments)     \n",
    "\n",
    "# fincier version\n",
    "GPT2_TOKENIZER_REGEX = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "pattern = GPT2_TOKENIZER_REGEX\n",
    "segments = regex.findall(pattern, string)\n",
    "print(segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPE Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class BPETokenizerParams:\n",
    "    \"\"\"All you need to specify a BPETokenizer.\"\"\"\n",
    "    vocab: dict[int, bytes]     # index -> bytes\n",
    "    merges: dict[tuple[int, int], int]  # index1,index2 -> new_index\n",
    "\n",
    "def merge(indices: list[int], pair: tuple[int, int], new_index: int) -> list[int]:\n",
    "    \"\"\"Return `indices`, but with all instances of `pair` replaced with `new_index`.\"\"\"\n",
    "    new_indcies = []\n",
    "    i = 0\n",
    "    while i < len(indices):\n",
    "        if i + 1 < len(indices) and (indices[i], indices[i + 1]) == pair:\n",
    "            new_indcies.append(new_index)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_indcies.append(indices[i])\n",
    "            i += 1\n",
    "    return new_indcies\n",
    "\n",
    "class BPETokenizer(Tokenizer):\n",
    "    \"\"\"BPE tokenizer given a set of merges and a vocabulary.\"\"\"\n",
    "    def __init__(self, params: BPETokenizerParams):\n",
    "        self.params = params\n",
    "\n",
    "    def encode(self, string: str) -> list[int]:\n",
    "        indices = list(map(int, string.encode(\"utf-8\")))\n",
    "        # Note: this is a very slow implementation\n",
    "        for pair, new_index in self.params.merges.items():\n",
    "            indices = merge(indices, pair, new_index)\n",
    "        return indices\n",
    "    \n",
    "    def decode(self, indices: list[int]) -> str:\n",
    "        bytes_list = list(map(self.params.vocab.get, indices))\n",
    "        string = b\"\".join(bytes_list).decode(\"utf-8\")\n",
    "        return string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bpe(string: str, num_merges: int) -> BPETokenizerParams:\n",
    "    # start with the list of bytes of string\n",
    "    indices = list(map(int, string.encode('utf-8')))\n",
    "    merges: dict[tuple[int, int], int] = {}\n",
    "    vocab: dict[int, bytes] = {x: bytes([x]) for x in range(256)}\n",
    "\n",
    "    for i in range(num_merges):\n",
    "        # count the number of occurrences of each pair of tokens\n",
    "        counts = defaultdict(int)\n",
    "        for index1, index2 in zip(indices, indices[1:]):\n",
    "            counts[(index1, index2)] += 1\n",
    "\n",
    "        # find the most common pair\n",
    "        pair = max(counts, key=counts.get)\n",
    "        index1, index2 = pair\n",
    "\n",
    "        # merge the pair\n",
    "        new_index = 256 + i\n",
    "        merges[pair] = new_index\n",
    "        vocab[new_index] = vocab[index1] + vocab[index2]\n",
    "        indices = merge(indices, pair, new_index)\n",
    "\n",
    "    return BPETokenizerParams(merges=merges, vocab=vocab)\n",
    "\n",
    "string = \"the cat in the hat\"  \n",
    "params = train_bpe(string, num_merges=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BPETokenizer(params)\n",
    "string = \"the quick brown fox\"  # @inspect string\n",
    "indices = tokenizer.encode(string)  # @inspect indices\n",
    "reconstructed_string = tokenizer.decode(indices)  # @inspect reconstructed_string\n",
    "assert string == reconstructed_string\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
