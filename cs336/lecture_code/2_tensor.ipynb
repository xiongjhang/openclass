{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import timeit\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "\n",
    "import numpy as np\n",
    "from jaxtyping import Float\n",
    "from einops import rearrange, einsum, reduce\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Memory is determined by the (i) number of values and (ii) data type of each value.\n",
    "def get_memory_usage(x: torch.Tensor):\n",
    "    return x.numel() * x.element_size()\n",
    "\n",
    "def get_promised_flop_per_sec(device: str, dtype: torch.dtype) -> float:\n",
    "    \"\"\"Return the peak FLOP/s for `device` operating on `dtype`.\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return 1\n",
    "    \n",
    "    properties = torch.cuda.get_device_properties(device)\n",
    "    if \"A100\" in properties.name:\n",
    "        # https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf\")\n",
    "        if dtype == torch.float32:\n",
    "            return 19.5e12\n",
    "        if dtype in (torch.bfloat16, torch.float16):\n",
    "            return 312e12\n",
    "        raise ValueError(f\"Unknown dtype: {dtype}\")\n",
    "    \n",
    "    if \"H100\" in properties.name:\n",
    "        # https://resources.nvidia.com/en-us-tensor-core/nvidia-tensor-core-gpu-datasheet\")\n",
    "        if dtype == torch.float32:\n",
    "            return 67.5e12\n",
    "        if dtype in (torch.bfloat16, torch.float16):\n",
    "            return 1979e12 / 2  # 1979 is for sparse, dense is half of that\n",
    "        raise ValueError(f\"Unknown dtype: {dtype}\")\n",
    "    \n",
    "    raise ValueError(f\"Unknown device: {device}\")\n",
    "\n",
    "def same_storage(x: torch.Tensor, y: torch.Tensor):\n",
    "    return x.untyped_storage().data_ptr() == y.untyped_storage().data_ptr()\n",
    "\n",
    "def time_matmul(a: torch.Tensor, b: torch.Tensor) -> float:\n",
    "    \"\"\"Return the number of seconds required to perform `a @ b`.\"\"\"\n",
    "    # Wait until previous CUDA threads are done\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    def run():\n",
    "        # Perform the operation\n",
    "        a @ b\n",
    "        # Wait until CUDA threads are done\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "            \n",
    "    # Time the operation `num_trials` times\n",
    "    num_trials = 5\n",
    "    total_time = timeit.timeit(run, number=num_trials)\n",
    "    return total_time / num_trials\n",
    "\n",
    "def get_device(index: int = 0) -> torch.device:\n",
    "    \"\"\"Try to use the GPU if possible, otherwise, use CPU.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(f\"cuda:{index}\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empty: tensor([[ 7.5551e+31,  1.8672e+25,  1.2709e+31,  4.5277e+21,  7.1561e+22,\n",
      "          9.4082e-39, -1.1160e+21,  2.5353e+30],\n",
      "        [ 2.2421e-44,  1.2747e-40,  0.0000e+00,  0.0000e+00,  2.7551e-40,\n",
      "          4.5918e-40,  1.0561e-38,  1.2864e-20],\n",
      "        [ 2.6585e+36,  1.6929e+17,  1.0757e+37, -1.0880e-19,  2.6893e+36,\n",
      "          3.5907e+13,  6.3384e+29,  5.5143e+11],\n",
      "        [ 4.1735e-39,  2.4832e-37,  7.9081e-41,  2.6568e+27,  1.6531e+19,\n",
      "          1.0903e+27,  2.5986e+11, -9.0072e+15]])\n",
      "trunc_normal: tensor([[-0.8814,  1.3793,  0.1921,  0.6386, -0.1770, -0.4021, -1.6637,  0.4388],\n",
      "        [-1.0273,  1.3479, -1.0692,  0.0289, -0.2132, -1.0884,  0.3821,  1.8391],\n",
      "        [-0.0670, -1.4548, -0.8951, -0.0703,  0.4263,  0.2505, -0.7096, -0.9426],\n",
      "        [ 0.7440, -1.6184, -1.1916,  0.0469,  0.0388, -0.8756,  0.6893, -1.3750]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1., 2, 3], [4, 5, 6]])\n",
    "x = torch.zeros(4, 8)  # 4x8 matrix of all zeros\n",
    "x = torch.ones(4, 8)  # 4x8 matrix of all ones \n",
    "x = torch.randn(4, 8)  # 4x8 matrix of iid Normal(0, 1) samples \n",
    "\n",
    "x = torch.empty(4, 8)  # 4x8 matrix of uninitialized values\n",
    "print(f\"empty: {x}\")\n",
    "nn.init.trunc_normal_(x, mean=0, std=1, a=-2, b=2)\n",
    "print(f\"trunc_normal: {x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fp32 \n",
    "x = torch.zeros(4, 8) \n",
    "assert x.dtype == torch.float32  # Default type\n",
    "assert x.numel() == 4 * 8\n",
    "assert x.element_size() == 4  # Float is 4 bytes\n",
    "assert get_memory_usage(x) == 4 * 8 * 4  # 128 bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fp16\n",
    "x = torch.zeros(4, 8, dtype=torch.float16)  # @inspect x\n",
    "assert x.element_size() == 2\n",
    "\n",
    "x = torch.tensor([1e-8], dtype=torch.float16)  # @inspect x\n",
    "assert x == 0  # Underflow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bf16\n",
    "x = torch.tensor([1e-8], dtype=torch.bfloat16)  # @inspect x\n",
    "assert x != 0  # No underflow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32: finfo(resolution=1e-06, min=-3.40282e+38, max=3.40282e+38, eps=1.19209e-07, smallest_normal=1.17549e-38, tiny=1.17549e-38, dtype=float32)\n",
      "float16: finfo(resolution=0.001, min=-65504, max=65504, eps=0.000976562, smallest_normal=6.10352e-05, tiny=6.10352e-05, dtype=float16)\n",
      "bfloat16: finfo(resolution=0.01, min=-3.38953e+38, max=3.38953e+38, eps=0.0078125, smallest_normal=1.17549e-38, tiny=1.17549e-38, dtype=bfloat16)\n"
     ]
    }
   ],
   "source": [
    "# compare the dynamic ranges and memory usage of the different data types\n",
    "float32_info = torch.finfo(torch.float32)  # @inspect float32_info\n",
    "float16_info = torch.finfo(torch.float16)  # @inspect float16_info\n",
    "bfloat16_info = torch.finfo(torch.bfloat16)  # @inspect bfloat16_info\n",
    "print(f\"float32: {float32_info}\")\n",
    "print(f\"float16: {float16_info}\")\n",
    "print(f\"bfloat16: {bfloat16_info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor on GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce RTX 4090, Compute Capability: 8.9, Memory: 23.65 GB\n",
      "Initial memory allocated on GPU: 0.00 MB\n",
      "Move the tensor to GPU memory (device 0).\n",
      "Or create a tensor directly on the GPU:\n",
      "Memory allocated after moving tensor to GPU: 0.01 MB\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(32, 32)\n",
    "assert x.device == torch.device(\"cpu\")\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"CUDA is not available. Please run this code on a machine with a GPU.\")\n",
    "\n",
    "num_gpus = torch.cuda.device_count()  # @inspect num_gpus\n",
    "for i in range(num_gpus):\n",
    "    properties = torch.cuda.get_device_properties(i)  # @inspect properties\n",
    "    print(f\"GPU {i}: {properties.name}, Compute Capability: {properties.major}.{properties.minor}, Memory: {properties.total_memory / (1024 ** 3):.2f} GB\")\n",
    "\n",
    "memory_allocated = torch.cuda.memory_allocated()  # @inspect memory_allocated\n",
    "print(f\"Initial memory allocated on GPU: {memory_allocated / (1024 ** 2):.2f} MB\")\n",
    "\n",
    "print(\"Move the tensor to GPU memory (device 0).\")\n",
    "y = x.to(\"cuda:0\")\n",
    "assert y.device == torch.device(\"cuda\", 0)\n",
    "\n",
    "print(\"Or create a tensor directly on the GPU:\")\n",
    "z = torch.zeros(32, 32, device=\"cuda:0\")\n",
    "\n",
    "new_memory_allocated = torch.cuda.memory_allocated()  # @inspect new_memory_allocated\n",
    "print(f\"Memory allocated after moving tensor to GPU: {new_memory_allocated / (1024 ** 2):.2f} MB\")\n",
    "memory_used = new_memory_allocated - memory_allocated  # @inspect memory_used\n",
    "assert memory_used == 2 * (32 * 32 * 4)  # 2 32x32 matrices of 4-byte floats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor storage\n",
    "x = torch.tensor([\n",
    "    [0., 1, 2, 3],\n",
    "    [4, 5, 6, 7],\n",
    "    [8, 9, 10, 11],\n",
    "    [12, 13, 14, 15],\n",
    "])\n",
    "\n",
    "# To go to the next row (dim 0), skip 4 elements in storage.\n",
    "assert x.stride(0) == 4\n",
    "\n",
    "# To go to the next column (dim 1), skip 1 element in storage.\n",
    "assert x.stride(1) == 1\n",
    "\n",
    "# To find an element:\n",
    "r, c = 1, 2\n",
    "index = r * x.stride(0) + c * x.stride(1)  # @inspect index\n",
    "assert index == 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original x:\n",
      "x shape: torch.Size([2, 3])\n",
      "x stride: (3, 1)\n",
      "After transpose:\n",
      "y shape: torch.Size([3, 2])\n",
      "y stride: (1, 3)\n"
     ]
    }
   ],
   "source": [
    "# tensor slicing\n",
    "x = torch.tensor([[1., 2, 3], [4, 5, 6]])\n",
    "\n",
    "# Get row 0:\n",
    "y = x[0]  # @inspect y\n",
    "assert torch.equal(y, torch.tensor([1., 2, 3]))\n",
    "assert same_storage(x, y)\n",
    "\n",
    "# Get column 1:\n",
    "y = x[:, 1]  # @inspect y\n",
    "assert torch.equal(y, torch.tensor([2, 5]))\n",
    "assert same_storage(x, y)\n",
    "\n",
    "# View 2x3 matrix as 3x2 matrix:\n",
    "y = x.transpose(1, 0)  # @inspect y\n",
    "assert torch.equal(y, torch.tensor([[1, 4], [2, 5], [3, 6]]))\n",
    "assert same_storage(x, y)\n",
    "\n",
    "# Check that mutating x also mutates y.\n",
    "x[0][0] = 100  # @inspect x, @inspect y\n",
    "assert y[0][0] == 100\n",
    "\n",
    "# Note that some views are non-contiguous entries, which means that further views aren't possible.\n",
    "x = torch.tensor([[1., 2, 3], [4, 5, 6]])  # @inspect x\n",
    "print(\"Original x:\")\n",
    "print(\"x shape:\", x.shape)\n",
    "print(\"x stride:\", x.stride())\n",
    "y = x.transpose(1, 0)  # @inspect y\n",
    "print(\"After transpose:\")\n",
    "print(\"y shape:\", y.shape)\n",
    "print(\"y stride:\", y.stride())\n",
    "assert not y.is_contiguous()\n",
    "try:\n",
    "    y.view(2, 3)\n",
    "    # print(\"After view:\")\n",
    "    # print(\"y shape:\", y.shape)\n",
    "    # print(\"y stride:\", y.stride())\n",
    "    assert False\n",
    "except RuntimeError as e:\n",
    "    assert \"view size is not compatible with input tensor's size and stride\" in str(e)\n",
    "\n",
    "# One can enforce a tensor to be contiguous first:\n",
    "y = x.transpose(1, 0).contiguous().view(2, 3)  # @inspect y\n",
    "# or using reshape\n",
    "# y = x.transpose(1, 0).reshape(2, 3)  # @inspect y\n",
    "assert not same_storage(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor elementwise\n",
    "x = torch.tensor([1, 4, 9])\n",
    "assert torch.equal(x.pow(2), torch.tensor([1, 16, 81]))\n",
    "assert torch.equal(x.sqrt(), torch.tensor([1, 2, 3]))\n",
    "assert torch.equal(x.rsqrt(), torch.tensor([1, 1 / 2, 1 / 3]))  # i -> 1/sqrt(x_i)\n",
    "\n",
    "assert torch.equal(x + x, torch.tensor([2, 8, 18]))\n",
    "assert torch.equal(x * 2, torch.tensor([2, 8, 18]))\n",
    "assert torch.equal(x / 0.5, torch.tensor([2, 8, 18]))\n",
    "\n",
    "# triu takes the upper triangular part of a matrix.\n",
    "x = torch.ones(3, 3).triu()  # @inspect x\n",
    "assert torch.equal(x, torch.tensor([\n",
    "    [1, 1, 1],\n",
    "    [0, 1, 1],\n",
    "    [0, 0, 1]],\n",
    "))\n",
    "# This is useful for computing an causal attention mask, where M[i, j] is the contribution of i to j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor matmul\n",
    "x = torch.ones(16, 32)\n",
    "w = torch.ones(32, 2)\n",
    "y = x @ w\n",
    "assert y.size() == torch.Size([16, 2])\n",
    "\n",
    "# In general, we perform operations for every example in a batch and token in a sequence.\n",
    "x = torch.ones(4, 8, 16, 32)\n",
    "w = torch.ones(32, 2)\n",
    "y = x @ w\n",
    "assert y.size() == torch.Size([4, 8, 16, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# motivation\n",
    "x = torch.ones(2, 2, 3)  # batch, sequence, hidden  @inspect x\n",
    "y = torch.ones(2, 2, 3)  # batch, sequence, hidden  @inspect y\n",
    "z = x @ y.transpose(-2, -1)  # batch, sequence, sequence  @inspect z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jax typing basic\n",
    "x = torch.ones(2, 2, 1, 3)  # batch seq heads hidden\n",
    "# New (jaxtyping) way\n",
    "# Note: this is just documentation (no enforcement).\n",
    "x: Float[torch.Tensor, \"batch seq heads hidden\"] = torch.ones(2, 2, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# einops einsum\n",
    "x: Float[torch.Tensor, \"batch seq1 hidden\"] = torch.ones(2, 3, 4)  # @inspect x\n",
    "y: Float[torch.Tensor, \"batch seq2 hidden\"] = torch.ones(2, 3, 4)  # @inspect y\n",
    "\n",
    "# old way\n",
    "z = x @ y.transpose(-2, -1)  # batch, sequence, sequence  @inspect z\n",
    "\n",
    "# new(einops) way\n",
    "# Dimensions that are not named in the output are summed over.\n",
    "z = einsum(x, y, \"batch seq1 hidden, batch seq2 hidden -> batch seq1 seq2\") # @inspect z\n",
    "# Or can use ... to represent broadcasting over any number of dimensions:\n",
    "z = einsum(x, y, \"... seq1 hidden, ... seq2 hidden -> ... seq1 seq2\")  # @inspect z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# einops reduce\n",
    "x: Float[torch.Tensor, \"batch seq hidden\"] = torch.ones(2, 3, 4)  # @inspect x\n",
    "\n",
    "# old way\n",
    "y = x.mean(dim=1)  # @inspect y\n",
    "# new(einops) way\n",
    "y = reduce(x, \"... hidden -> ...\", \"mean\")  # @inspect y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# einsum rearrange\n",
    "x: Float[torch.Tensor, \"batch seq total_hidden\"] = torch.ones(2, 3, 8)  # @inspect x\n",
    "w: Float[torch.Tensor, \"hidden1 hidden2\"] = torch.ones(4, 4)\n",
    "\n",
    "# Break up total_hidden into two dimensions (heads and hidden1):\n",
    "x = rearrange(x, \"... (heads hidden1) -> ... heads hidden1\", heads=2)  # @inspect x\n",
    "\n",
    "# Perform the transformation by w:\n",
    "x = einsum(x, w, \"... heads hidden1, hidden1 hidden2 -> ... heads hidden2\")  # @inspect x\n",
    "\n",
    "# Combine heads and hidden2 back together:\n",
    "x = rearrange(x, \"... heads hidden2 -> ... (heads hidden2)\")  # @inspect x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Operation flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_flop_per_sec: 51.54 TFLOP/s\n"
     ]
    }
   ],
   "source": [
    "# Linear model\n",
    "# As motivation, suppose you have a linear model.\n",
    "# We have n points\n",
    "# Each point is d-dimsional\n",
    "# The linear model maps each d-dimensional vector to a k outputs\n",
    "if torch.cuda.is_available():\n",
    "    B = 16384  # Number of points\n",
    "    D = 32768  # Dimension\n",
    "    K = 8192   # Number of outputs\n",
    "else:\n",
    "    B = 1024\n",
    "    D = 256\n",
    "    K = 64\n",
    "\n",
    "device = get_device()\n",
    "x = torch.ones(B, D, device=device)\n",
    "w = torch.randn(D, K, device=device)\n",
    "y = x @ w\n",
    "\n",
    "actual_num_flops = 2 * B * D * K  # @inspect actual_num_flops\n",
    "actual_time = time_matmul(x, w)  # @inspect actual_time\n",
    "actual_flop_per_sec = actual_num_flops / actual_time  # @inspect actual_flop_per_sec\n",
    "print(f\"actual_flop_per_sec: {actual_flop_per_sec / 1e12:.2f} TFLOP/s\")  # @inspect actual_flop_per_sec\n",
    "# promised_flop_per_sec = get_promised_flop_per_sec(device, x.dtype)  # @inspect promised_flop_per_sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bf16_actual_flop_per_sec: 148.85 TFLOP/s\n"
     ]
    }
   ],
   "source": [
    "# model flops utilization(MFU)\n",
    "# mfu = actual_flop_per_sec / promised_flop_per_sec  # @inspect mfu\n",
    "\n",
    "x = x.to(torch.bfloat16)\n",
    "w = w.to(torch.bfloat16)\n",
    "bf16_actual_time = time_matmul(x, w)  # @inspect bf16_actual_time\n",
    "bf16_actual_flop_per_sec = actual_num_flops / bf16_actual_time  # @inspect bf16_actual_flop_per_sec\n",
    "print(f\"bf16_actual_flop_per_sec: {bf16_actual_flop_per_sec / 1e12:.2f} TFLOP/s\")  # @inspect bf16_actual_flop_per_sec\n",
    "# bf16_promised_flop_per_sec = get_promised_flop_per_sec(device, x.dtype)  # @inspect bf16_promised_flop_per_sec\n",
    "# bf16_mfu = bf16_actual_flop_per_sec / bf16_promised_flop_per_sec  # @inspect bf16_mfu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_145211/3762839567.py:9: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181081/work/build/aten/src/ATen/core/TensorBody.h:489.)\n",
      "  assert loss.grad is None\n",
      "/tmp/ipykernel_145211/3762839567.py:10: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181081/work/build/aten/src/ATen/core/TensorBody.h:489.)\n",
      "  assert pred_y.grad is None\n"
     ]
    }
   ],
   "source": [
    "# forward pass\n",
    "x = torch.tensor([1., 2, 3])\n",
    "w = torch.tensor([1., 1, 1], requires_grad=True)  # Want gradient\n",
    "pred_y = x @ w\n",
    "loss = 0.5 * (pred_y - 5).pow(2)\n",
    "\n",
    "# backward pass\n",
    "loss.backward()\n",
    "assert loss.grad is None\n",
    "assert pred_y.grad is None\n",
    "assert x.grad is None\n",
    "assert torch.equal(w.grad, torch.tensor([1, 2, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us do count the FLOPs for computing gradients.\n",
    "# Revisit our linear model\n",
    "if torch.cuda.is_available():\n",
    "    B = 16384  # Number of points\n",
    "    D = 32768  # Dimension\n",
    "    K = 8192   # Number of outputs\n",
    "else:\n",
    "    B = 1024\n",
    "    D = 256\n",
    "    K = 64\n",
    "\n",
    "device = get_device()\n",
    "x = torch.ones(B, D, device=device)\n",
    "w1 = torch.randn(D, D, device=device, requires_grad=True)\n",
    "w2 = torch.randn(D, K, device=device, requires_grad=True)\n",
    "\n",
    "# Model: x --w1--> h1 --w2--> h2 -> loss\n",
    "h1 = x @ w1\n",
    "h2 = h1 @ w2\n",
    "loss = h2.pow(2).mean()\n",
    "# foward flops\n",
    "num_forward_flops = (2 * B * D * D) + (2 * B * D * K)\n",
    "\n",
    "h1.retain_grad()  # For debugging\n",
    "h2.retain_grad()  # For debugging\n",
    "loss.backward()\n",
    "\n",
    "# How many FLOPs is running the backward pass?\n",
    "# Invoke the chain rule.\n",
    "num_backward_flops = 0\n",
    "\n",
    "# w2.grad[j,k] = sum_i h1[i,j] * h2.grad[i,k]\n",
    "assert w2.grad.size() == torch.Size([D, K])\n",
    "assert h1.size() == torch.Size([B, D])\n",
    "assert h2.grad.size() == torch.Size([B, K])\n",
    "# For each (i, j, k), multiply and add.\n",
    "num_backward_flops += 2 * B * D * K\n",
    "\n",
    "# h1.grad[i,j] = sum_k w2[j,k] * h2.grad[i,k]\n",
    "assert h1.grad.size() == torch.Size([B, D])\n",
    "assert w2.size() == torch.Size([D, K])\n",
    "assert h2.grad.size() == torch.Size([B, K])\n",
    "# For each (i, j, k), multiply and add.\n",
    "num_backward_flops += 2 * B * D * K\n",
    "\n",
    "# This was for just w2 (D*K parameters).\n",
    "# Can do it for w1 (D*D parameters) as well (though don't need x.grad).\n",
    "num_backward_flops += (2 + 2) * B * D * D "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
